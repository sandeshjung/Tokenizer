# ğŸ”– Byte-Pair Encoding (BPE) Tokenizer

A compact Byte-Pair Encoding (BPE) toolkit inspired by Andrew Karpathyâ€™s hands-on style. This repository shows you how to:

- Turn raw text into integer token sequences (and back).
- Build a subword vocabulary by iteratively merging the most frequent symbol pairs.
- Explore implementations in C, Python, and Jupyter Notebook.


## ğŸš€ What Is Tokenization?

Tokenization is all about breaking text into pieces your model can understand. Think of each character (or word) getting a unique ID. Once you have that mapping, you can convert any sentence into a series of numbersâ€”and back againâ€”like so:

```python
chars = sorted(set(text))                    # every unique character
stoi  = {ch: i for i, ch in enumerate(chars)}  # char â†’ integer
itos  = {i: ch for ch, i in stoi.items()}     # integer â†’ char

def encode(s):
    return [stoi[c] for c in s]

def decode(l):
    return ''.join(itos[i] for i in l)
```

With just these few lines, youâ€™ve built a **character-level** tokenizer. Languages are full of common substringsâ€”â€œing,â€ â€œtion,â€, etc. which would have their own tokens. This is where Byte-Pair Encoding (BPE) shines.

## âœ‚ï¸ How Byte-Pair Encoding (BPE) Works

BPE discovers frequent subword units by repeating a simple loop:

1. **Count pairs**: Scan your token sequence and count repetition of each adjacent pair `(u, v)` appears.
2. **Select highest-frequency pair**: Find the pair with the highest count.
3. **Merge**: Assign that pair a new token ID and replace every occurrence with this new ID.
4. **Repeat**: Do this `N` times to end up with a vocabulary of size `256 + N` (256 initial bytes plus `N` merges).

This process handles words (by breaking them into smaller pieces) and common words (by merging them into single tokens).

---

## ğŸ§® Mathematical Explanation of BPE

- Start with:
  V_0 = \{0,1,\ldots,255\}, \quad T = [t_1,t_2,\ldots,t_M], \; t_j \in V_0.

- At each iteration (i = 1\ldots N):
  1. Count frequencies:
     f_i(u,v) = |\{\,j \,|\, (T_j, T_{j+1}) = (u,v)\}|.
  2. Select highest-frequency pair:
     (u^*, v^*) = \arg\max_{(u,v)} f_i(u,v).
  3. Merge:
       w_i = 256 + i, \quad
       T \gets \text{replace every }(u^*, v^*)\text{ in }T\text{ with }w_i, \quad
       V_i = V_{i-1} \cup \{w_i\}.

After (N) merges, your final vocabulary is:

|V_N| = 256 + N.

---

## ğŸ“ Repository Structure

```plaintext
tokenizer/                     # root folder
â”œâ”€â”€ bpe_playground/            # demo apps
â”‚   â”œâ”€â”€ app/                   # Flask backend
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ app.py
â”‚   â”‚   â”œâ”€â”€ bpe.py
â”‚   â”‚   â””â”€â”€ routes.py
â”‚   â””â”€â”€ streamlit_app/         # Streamlit frontend
â”‚       â””â”€â”€ app.py
â”œâ”€â”€ en/                        # English BPE examples
â”‚   â””â”€â”€ bpe/
â”‚       â”œâ”€â”€ bpe.c              # tokenizer implementation using C
â”‚       â””â”€â”€ bpe.ipynb
â”œâ”€â”€ ne/                        # Nepali tokenizer
â”‚   â”œâ”€â”€ text.txt
â””   â””â”€â”€ tokenizer_nepali.py

```
